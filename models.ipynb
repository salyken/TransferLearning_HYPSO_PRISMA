{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e463736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a84a51",
   "metadata": {},
   "source": [
    "### Model 1 and 3: 2D and VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HyperspectralTransferCNN(nn.Module):\n",
    "    def __init__(self, input_bands=63, num_classes=3, backbone_type='vgg16'):\n",
    "        super(HyperspectralTransferCNN, self).__init__()\n",
    "\n",
    "        # Step 1: Reduce hyperspectral input to 3 channels\n",
    "        self.mapping = nn.Conv2d(input_bands, 3, kernel_size=1)\n",
    "\n",
    "        # Step 2: Backbone\n",
    "        if backbone_type == 'vgg16': # VGG-16 model\n",
    "            backbone = models.vgg16_bn(pretrained=True)\n",
    "            self.features = backbone.features\n",
    "            self.blocks = self.features  # Alias to make training logic consistent\n",
    "            in_features = 512\n",
    "\n",
    "        elif backbone_type == 'resnet34': # Not tested\n",
    "            backbone = models.resnet34(pretrained=True)\n",
    "            self.features = nn.Sequential(*list(backbone.children())[:-2])\n",
    "            in_features = 512\n",
    "        elif backbone_type == 'custom': # 2D model\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.blocks = self.features  # Alias to make training logic consistent\n",
    "            in_features = 64  # Output channels from last conv layer\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backbone. Choose 'vgg16', 'resnet34', or 'custom'.\")\n",
    "\n",
    "        # Step 3: Global pooling (works for any input size)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Step 4: Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mapping(x)\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab153e99",
   "metadata": {},
   "source": [
    "### Model 2: Hybrid with Projection Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedHybrid3D2DCNN_v2(nn.Module):\n",
    "    def __init__(self, in_bands=63, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Spectral–spatial 3D encoder\n",
    "        self.encoder3d = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.BatchNorm3d(16), nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=1, stride=(2, 1, 1)),\n",
    "            nn.BatchNorm3d(32), nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(0.1),\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1, stride=(2, 1, 1)),\n",
    "            nn.BatchNorm3d(64), nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(0.1),\n",
    "            nn.AdaptiveAvgPool3d((1, None, None))  # retain spatial, compress spectral\n",
    "        )\n",
    "\n",
    "        # Flatten spectral dim\n",
    "        self.project_2d = nn.Conv2d(64, 32, kernel_size=1)\n",
    "\n",
    "        # 2D convolutional head (deeper)\n",
    "        self.features2d = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "        )\n",
    "        # self.blocks = self.features2d  # Alias to make training logic consistent\n",
    "\n",
    "        # Optional self-attention (can remove for simplicity)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Conv2d(128, 32, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 128, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, Bands, H, W)\n",
    "        x = x.unsqueeze(1)            # (B,1,Bands,H,W)\n",
    "        x = self.encoder3d(x)         # (B,64,1,H,W)\n",
    "        x = x.squeeze(2)              # (B,64,H,W)\n",
    "        x = self.project_2d(x)        # (B,32,H,W)\n",
    "        x = self.features2d(x)        # (B,128,H,W)\n",
    "\n",
    "        # Self-attention (optional)\n",
    "        attn = self.attention(x)      # (B,128,1,1)\n",
    "        x = x * attn                  # Channel-wise attention\n",
    "\n",
    "        x = self.pool(x)              # (B,128,1,1)\n",
    "        return self.classifier(x)     # (B,num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52725a3",
   "metadata": {},
   "source": [
    "### Model 2: Hybrid with Mapping Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f74e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImprovedHybrid3D2DCNN_v3(nn.Module):\n",
    "#     def __init__(self, in_bands=47, num_classes=3, mapped_bands=63):\n",
    "#         super().__init__()\n",
    "#         self.in_bands = in_bands\n",
    "#         self.mapped_bands = mapped_bands\n",
    "\n",
    "#         # Learned spectral mapping: projects any-band input to 63-band space\n",
    "#         self.mapping = nn.Linear(in_bands, mapped_bands)\n",
    "\n",
    "#         # Spectral–spatial 3D encoder (expecting mapped_bands as spectral depth)\n",
    "#         self.encoder3d = nn.Sequential(\n",
    "#             nn.Conv3d(1, 16, kernel_size=(3, 3, 3), padding=1),\n",
    "#             nn.BatchNorm3d(16), nn.ReLU(inplace=True),\n",
    "#             nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=1, stride=(2, 1, 1)),\n",
    "#             nn.BatchNorm3d(32), nn.ReLU(inplace=True),\n",
    "#             nn.Dropout3d(0.1),\n",
    "#             nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1, stride=(2, 1, 1)),\n",
    "#             nn.BatchNorm3d(64), nn.ReLU(inplace=True),\n",
    "#             nn.Dropout3d(0.1),\n",
    "#             nn.AdaptiveAvgPool3d((1, None, None))  # retain spatial, compress spectral\n",
    "#         )\n",
    "\n",
    "#         # Flatten spectral dim\n",
    "#         self.project_2d = nn.Conv2d(64, 32, kernel_size=1)\n",
    "\n",
    "#         # 2D convolutional head\n",
    "#         self.features2d = nn.Sequential(\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "#             nn.Dropout2d(0.2),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "#             nn.Dropout2d(0.2),\n",
    "#         )\n",
    "\n",
    "#         # Optional self-attention\n",
    "#         self.attention = nn.Sequential(\n",
    "#             nn.AdaptiveAvgPool2d((1, 1)),\n",
    "#             nn.Conv2d(128, 32, kernel_size=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(32, 128, kernel_size=1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#         self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(64, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (B, Bands, H, W) — raw input\n",
    "#         B, C, H, W = x.shape  # C = in_bands\n",
    "\n",
    "#         # 1. Spectral mapping: (B, C, H, W) → (B, 63, H, W)\n",
    "#         x = x.permute(0, 2, 3, 1).reshape(-1, C)        # (B*H*W, in_bands)\n",
    "#         x = self.mapping(x)                            # (B*H*W, mapped_bands)\n",
    "#         x = x.view(B, H, W, self.mapped_bands).permute(0, 3, 1, 2)  # (B, mapped_bands, H, W)\n",
    "\n",
    "#         # 2. Continue normal flow\n",
    "#         x = x.unsqueeze(1)            # (B, 1, mapped_bands, H, W)\n",
    "#         x = self.encoder3d(x)         # (B, 64, 1, H, W)\n",
    "#         x = x.squeeze(2)              # (B, 64, H, W)\n",
    "#         x = self.project_2d(x)        # (B, 32, H, W)\n",
    "#         x = self.features2d(x)        # (B, 128, H, W)\n",
    "\n",
    "#         attn = self.attention(x)      # (B, 128, 1, 1)\n",
    "#         x = x * attn                  # attention-weighted features\n",
    "\n",
    "#         x = self.pool(x)              # (B, 128, 1, 1)\n",
    "#         return self.classifier(x)     # (B, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852fc3dd",
   "metadata": {},
   "source": [
    "### Model 4: VGG-16 sSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e39c5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialSEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SpatialSEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        squeeze = F.adaptive_avg_pool2d(x, 1).view(B, C)\n",
    "        excitation = torch.sigmoid(self.fc2(F.relu(self.fc1(squeeze)))).view(B, C, 1, 1)\n",
    "        return x * excitation\n",
    "\n",
    "\n",
    "class VGG16WithAttention(nn.Module):\n",
    "    def __init__(self, input_bands=63, num_classes=3, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1x1 conv to reduce hyperspectral to 3-channel RGB equivalent\n",
    "        self.mapping = nn.Conv2d(input_bands, 3, kernel_size=1)\n",
    "\n",
    "        # Load pretrained VGG16 backbone\n",
    "        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        layers = list(vgg.features.children())\n",
    "\n",
    "        # Divide into 5 blocks as in VGG\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.attentions = nn.ModuleList()\n",
    "        self.pooling = nn.ModuleList()\n",
    "\n",
    "        cfg = [6, 13, 23, 33, 43]  # Layer indices for MaxPool\n",
    "        in_channels_list = [64, 128, 256, 512, 512]\n",
    "\n",
    "        prev = 0\n",
    "        for idx, end in enumerate(cfg):\n",
    "            block = nn.Sequential(*layers[prev:end - 1])  # up to before MaxPool\n",
    "            self.blocks.append(block)\n",
    "            self.attentions.append(SpatialSEBlock(in_channels_list[idx]))\n",
    "            self.pooling.append(layers[end - 1])  # the MaxPool itself\n",
    "            prev = end\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mapping(x)  # From hyperspectral input to 3-channel\n",
    "\n",
    "        for block, attn, pool in zip(self.blocks, self.attentions, self.pooling):\n",
    "            x = block(x)\n",
    "            x = attn(x)  # Attention after block, before pooling\n",
    "            x = pool(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae71ccb",
   "metadata": {},
   "source": [
    "### Model 5: VGG-16 CBAM with Projection Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c34df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        # Channel attention\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction_ratio, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction_ratio, channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid_channel = nn.Sigmoid()\n",
    "\n",
    "        # Spatial attention\n",
    "        self.conv_spatial = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel Attention\n",
    "        b, c, _, _ = x.size()\n",
    "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
    "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
    "        scale = self.sigmoid_channel(avg_out + max_out).view(b, c, 1, 1)\n",
    "        x = x * scale\n",
    "\n",
    "        # Spatial Attention\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = x * self.sigmoid_spatial(self.conv_spatial(torch.cat([avg_out, max_out], dim=1)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpectralSpatialInputBlock(nn.Module):\n",
    "    def __init__(self, in_channels=47, mid_channels=16):\n",
    "        super(SpectralSpatialInputBlock, self).__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mapping(x)\n",
    "\n",
    "\n",
    "class VGG16WithCBAM(nn.Module):\n",
    "    def __init__(self, num_classes=3, in_channels=47):\n",
    "        super(VGG16WithCBAM, self).__init__()\n",
    "\n",
    "        self.input_block = SpectralSpatialInputBlock(in_channels=in_channels, mid_channels=16)\n",
    "\n",
    "        def make_block(in_c, out_c, num_convs):\n",
    "            layers = []\n",
    "            for _ in range(num_convs):\n",
    "                layers.append(nn.Conv2d(in_c, out_c, kernel_size=3, padding=1))\n",
    "                layers.append(nn.BatchNorm2d(out_c))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_c = out_c\n",
    "            layers.append(CBAM(out_c))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.block1 = make_block(16, 64, 2)\n",
    "        self.block2 = make_block(64, 128, 2)\n",
    "        self.block3 = make_block(128, 256, 3)\n",
    "        self.block4 = make_block(256, 512, 3)\n",
    "        self.block5 = make_block(512, 512, 3)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_block(x)\n",
    "        x = self.block1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.block2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.block3(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.block4(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.block5(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.pool(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36db50e",
   "metadata": {},
   "source": [
    "### Model 5: VGG-16 CBAM with Mapping Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e38eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class CBAM(nn.Module):\n",
    "#     def __init__(self, channels, reduction_ratio=16, kernel_size=7):\n",
    "#         super(CBAM, self).__init__()\n",
    "#         # Channel attention\n",
    "#         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(channels, channels // reduction_ratio, bias=False),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(channels // reduction_ratio, channels, bias=False)\n",
    "#         )\n",
    "#         self.sigmoid_channel = nn.Sigmoid()\n",
    "\n",
    "#         # Spatial attention\n",
    "#         self.conv_spatial = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "#         self.sigmoid_spatial = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Channel Attention\n",
    "#         b, c, _, _ = x.size()\n",
    "#         avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
    "#         max_out = self.fc(self.max_pool(x).view(b, c))\n",
    "#         scale = self.sigmoid_channel(avg_out + max_out).view(b, c, 1, 1)\n",
    "#         x = x * scale\n",
    "\n",
    "#         # Spatial Attention\n",
    "#         avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "#         max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "#         x = x * self.sigmoid_spatial(self.conv_spatial(torch.cat([avg_out, max_out], dim=1)))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class SpectralSpatialInputBlock(nn.Module):\n",
    "#     def __init__(self, in_channels=47, mid_channels=16):\n",
    "#         super(SpectralSpatialInputBlock, self).__init__()\n",
    "#         self.mapping = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, mid_channels, kernel_size=1),\n",
    "#             nn.BatchNorm2d(mid_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(mid_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.mapping(x)\n",
    "\n",
    "\n",
    "# class VGG16WithCBAM(nn.Module):\n",
    "#     def __init__(self, num_classes=3, in_channels=47):  # Default in_channels = 47\n",
    "#         super(VGG16WithCBAM, self).__init__()\n",
    "\n",
    "#         # Define the projection layer to reduce channels from other values to 47\n",
    "#         self.input_projection = nn.Conv2d(in_channels, 47, kernel_size=1) if in_channels != 47 else None\n",
    "\n",
    "#         # Define the rest of the model\n",
    "#         self.input_block = SpectralSpatialInputBlock(in_channels=47, mid_channels=16)\n",
    "\n",
    "#         def make_block(in_c, out_c, num_convs):\n",
    "#             layers = []\n",
    "#             for _ in range(num_convs):\n",
    "#                 layers.append(nn.Conv2d(in_c, out_c, kernel_size=3, padding=1))\n",
    "#                 layers.append(nn.BatchNorm2d(out_c))\n",
    "#                 layers.append(nn.ReLU(inplace=True))\n",
    "#                 in_c = out_c\n",
    "#             layers.append(CBAM(out_c))\n",
    "#             return nn.Sequential(*layers)\n",
    "\n",
    "#         self.block1 = make_block(16, 64, 2)\n",
    "#         self.block2 = make_block(64, 128, 2)\n",
    "#         self.block3 = make_block(128, 256, 3)\n",
    "#         self.block4 = make_block(256, 512, 3)\n",
    "#         self.block5 = make_block(512, 512, 3)\n",
    "\n",
    "#         self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # If input_channels is not 47, apply the 1x1 convolution to project input channels\n",
    "#         if self.input_projection is not None:\n",
    "#             x = self.input_projection(x)\n",
    "\n",
    "#         # Pass through the input block and subsequent layers\n",
    "#         x = self.input_block(x)\n",
    "#         x = self.block1(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = self.block2(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = self.block3(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = self.block4(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = self.block5(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = self.pool(x)\n",
    "#         return self.classifier(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
