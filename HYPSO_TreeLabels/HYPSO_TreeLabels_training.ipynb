{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2479f34c",
   "metadata": {},
   "source": [
    "### Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2621f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose available CUDAs for parallell computing\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2,5\"\n",
    "print(\"This notebook's PID:\", os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "389f7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "\n",
    "# Import models\n",
    "import import_ipynb\n",
    "from models import HyperspectralTransferCNN, ImprovedHybrid3D2DCNN_v2, VGG16WithAttention, VGG16WithCBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372a4291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab5394",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5329be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPSO paths\n",
    "cube_path = '/home/salyken/PRISMA/HYPSO_data/cube'\n",
    "labels_path = '/home/salyken/PRISMA/HYPSO_data/labels'\n",
    "list_path = '/home/salyken/PRISMA/HYPSO_data/list/hypso_labels.xlsx'\n",
    "split_save_path = '/home/salyken/PRISMA/HYPSO_data/list/hypso_train_val_test_split.csv'\n",
    "\n",
    "cube_files = sorted([f for f in os.listdir(cube_path) if f.endswith('.npy')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d68165",
   "metadata": {},
   "source": [
    "### Investigate HYPSO-TreeLabels Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b764bdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened Excel file: /home/salyken/PRISMA/HYPSO_data/list/hypso_labels.xlsx\n"
     ]
    }
   ],
   "source": [
    "def file_identifier(file_path): \n",
    "\n",
    "        # file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        if file_path.endswith(\".xlsx\"):\n",
    "            df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "            print(f\"Opened Excel file: {file_path}\")\n",
    "            return df\n",
    "    \n",
    "        elif file_path.endswith(\".tif\"):\n",
    "            with rasterio.open(file_path) as src:\n",
    "                print(f\"Opened TIFF file: {file_path}, Shape:\", src.read(3).shape)\n",
    "                img = src.read(1)  # Read the first band (1-based index)\n",
    "                \n",
    "            # Display the image\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "            plt.colorbar()\n",
    "            plt.title(\"GeoTIFF - Single Band\")\n",
    "            plt.xlabel(\"Width (X)\")\n",
    "            plt.ylabel(\"Height (Y)\")\n",
    "            plt.show()\n",
    "    \n",
    "        else:\n",
    "            print(f\"Skipping unknown file: {file_path}\")\n",
    "\n",
    "df_excel = file_identifier(list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd9ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus only on the relevant tree species\n",
    "species = ['spruce', 'pine', 'deciduous']\n",
    "\n",
    "# Sum counts across the full dataset\n",
    "total_counts = df_excel[species].sum()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.bar(total_counts.index, total_counts.values, color=[(180/255, 100/255, 20/255, 0.8), (60/255, 180/255, 220/255, 0.8), (200/255, 100/255, 220/255, 0.8)])\n",
    "plt.title('HYPSO-TreeLabels: Overall Tree species Class Distribution')\n",
    "plt.xlabel('Tree Species')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf6906",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bandwise_norm_from_cubes(cube_dir, train_cube_list, drop_first_n_bands=3):\n",
    "    total_sum = None\n",
    "    total_sum_sq = None\n",
    "    total_pixels = 0\n",
    "\n",
    "    for fname in tqdm(train_cube_list, desc=\"Computing HYPSO band stats\"):\n",
    "        cube = np.load(os.path.join(cube_dir, fname))  # (H, W, Bands)\n",
    "        cube = cube[:, :, drop_first_n_bands:]  # Drop bad bands if needed ‚Üí (H, W, usable_Bands)\n",
    "\n",
    "        # Reshape to (Bands, H*W)\n",
    "        cube = np.transpose(cube, (2, 0, 1)).reshape(cube.shape[2], -1)\n",
    "\n",
    "        sum_ = np.sum(cube, axis=1)      # (Bands,)\n",
    "        sum_sq = np.sum(cube**2, axis=1) # (Bands,)\n",
    "        pixels = cube.shape[1]\n",
    "\n",
    "        if total_sum is None:\n",
    "            total_sum = sum_\n",
    "            total_sum_sq = sum_sq\n",
    "        else:\n",
    "            total_sum += sum_\n",
    "            total_sum_sq += sum_sq\n",
    "\n",
    "        total_pixels += pixels\n",
    "\n",
    "    band_means = total_sum / total_pixels\n",
    "    band_stds = np.sqrt((total_sum_sq / total_pixels) - (band_means ** 2))\n",
    "    \n",
    "    print(\" Band statistics computed.\")\n",
    "    return band_means, band_stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4789441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bandwise_norm_from_projected_cubes(cube_dir, train_cube_list, projection_matrix, drop_first_n_bands=3):\n",
    "    total_sum = None\n",
    "    total_sum_sq = None\n",
    "    total_pixels = 0\n",
    "\n",
    "    for fname in tqdm(train_cube_list, desc=\"Computing band stats after projection\"):\n",
    "        cube = np.load(os.path.join(cube_dir, fname))  # (H, W, Bands)\n",
    "        cube = cube[:, :, drop_first_n_bands:]  # Drop bad bands ‚Üí (H, W, 117)\n",
    "\n",
    "        # Apply projection: (63 x 117) @ (H x W x 117) ‚Üí (63 x H x W)\n",
    "        projected = np.tensordot(projection_matrix, cube, axes=([1], [2]))  # (63, H, W)\n",
    "        projected = np.moveaxis(projected, 0, -1)  # (H, W, 63)\n",
    "\n",
    "        # Reshape to (Bands, H*W)\n",
    "        flat = np.transpose(projected, (2, 0, 1)).reshape(projected.shape[2], -1)\n",
    "\n",
    "        sum_ = np.sum(flat, axis=1)      # (63,)\n",
    "        sum_sq = np.sum(flat**2, axis=1) # (63,)\n",
    "        pixels = flat.shape[1]\n",
    "\n",
    "        if total_sum is None:\n",
    "            total_sum = sum_\n",
    "            total_sum_sq = sum_sq\n",
    "        else:\n",
    "            total_sum += sum_\n",
    "            total_sum_sq += sum_sq\n",
    "\n",
    "        total_pixels += pixels\n",
    "\n",
    "    band_means = total_sum / total_pixels\n",
    "    band_stds = np.sqrt((total_sum_sq / total_pixels) - (band_means ** 2))\n",
    "\n",
    "    print(\" Projected band statistics computed.\")\n",
    "    return band_means, band_stds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ae971",
   "metadata": {},
   "source": [
    "### Create Patch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d10899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_patch_datasets(\n",
    "    cube_dir, label_dir, cube_list,\n",
    "    band_means=None, band_stds=None,\n",
    "    patch_size=33, stride=4,\n",
    "    train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,\n",
    "    seed=42,\n",
    "    projection_matrix=None,\n",
    "    majority_label=False  # toggleable for center- vs mode-based labeling scheme\n",
    "):\n",
    "    class HYPSOPatchDataset(Dataset):\n",
    "        def __init__(self, cube_data, index_map, band_means, band_stds, patch_size=5, augment=False, majority_label=False):\n",
    "            self.cube_data = cube_data\n",
    "            self.index_map = index_map\n",
    "            self.band_means = band_means\n",
    "            self.band_stds = band_stds\n",
    "            self.patch_size = patch_size\n",
    "            self.half = patch_size // 2\n",
    "            self.augment = augment\n",
    "            self.majority_label = majority_label  # store flag\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.index_map)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            data = self.index_map[idx]\n",
    "            if self.majority_label:\n",
    "                cube_idx, i, j, majority = data\n",
    "                label_val = majority - 1  # already validated\n",
    "            else:\n",
    "                cube_idx, i, j = data\n",
    "                cube, label = self.cube_data[cube_idx]\n",
    "                raw_label = int(label[i, j])\n",
    "                if raw_label not in (1, 2, 3):\n",
    "                    raise ValueError(f\"Invalid label {raw_label} at index {idx}\")\n",
    "                label_val = raw_label - 1\n",
    "\n",
    "            cube, _ = self.cube_data[cube_idx]\n",
    "            patch = cube[\n",
    "                i - self.half:i + self.half + 1,\n",
    "                j - self.half:j + self.half + 1,\n",
    "                :\n",
    "            ]\n",
    "            patch = np.transpose(patch, (2, 0, 1))\n",
    "            patch = torch.tensor(patch, dtype=torch.float32)\n",
    "\n",
    "            if self.band_means is not None and self.band_stds is not None:\n",
    "                mean = torch.tensor(self.band_means[:, None, None], dtype=torch.float32)\n",
    "                std = torch.tensor(self.band_stds[:, None, None], dtype=torch.float32)\n",
    "                patch = (patch - mean) / (std + 1e-6)\n",
    "\n",
    "            if self.augment:\n",
    "                patch = self.apply_augmentations(patch)\n",
    "\n",
    "            return patch, torch.tensor(label_val).long()\n",
    "\n",
    "        def apply_augmentations(self, x):\n",
    "            if torch.rand(1) < 0.5:\n",
    "                x = torch.flip(x, dims=[1])\n",
    "            if torch.rand(1) < 0.5:\n",
    "                x = torch.flip(x, dims=[2])\n",
    "            if torch.rand(1) < 0.5:\n",
    "                x = torch.rot90(x, k=1, dims=[1, 2])\n",
    "            if torch.rand(1) < 0.5:\n",
    "                x += torch.randn_like(x) * 0.01\n",
    "            return x\n",
    "\n",
    "    # Step 1: preload cubes\n",
    "    cube_data = []\n",
    "    for fname in cube_list:\n",
    "        cube = np.load(os.path.join(cube_dir, fname))[:, :, 3:]\n",
    "        if projection_matrix is not None:\n",
    "            cube = np.tensordot(projection_matrix, cube, axes=([1], [2]))\n",
    "            cube = np.moveaxis(cube, 0, -1)\n",
    "\n",
    "        label = np.loadtxt(\n",
    "            os.path.join(label_dir, fname.replace('_l1d_cube.npy', '_labels.csv')),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        cube_data.append((cube, label))\n",
    "\n",
    "    # Step 2: collect forest patches\n",
    "    class_map = defaultdict(list)\n",
    "    half = patch_size // 2\n",
    "\n",
    "    for cube_idx, (cube, label) in enumerate(tqdm(cube_data, desc=\"Indexing patches\")):\n",
    "        h, w = label.shape\n",
    "        for i in range(half, h - half, stride):\n",
    "            for j in range(half, w - half, stride):\n",
    "                if majority_label:\n",
    "                    patch_labels = label[i - half:i + half + 1, j - half:j + half + 1]\n",
    "                    valid = patch_labels[np.isin(patch_labels, [1, 2, 3])]\n",
    "                    if valid.size > 0:\n",
    "                        majority = mode(valid, axis=None).mode.item()\n",
    "                        if majority in (1, 2, 3):\n",
    "                            class_map[majority - 1].append((cube_idx, i, j, majority))  # include majority\n",
    "                else:\n",
    "                    class_label = label[i, j]\n",
    "                    if class_label in (1, 2, 3):\n",
    "                        class_map[class_label - 1].append((cube_idx, i, j))\n",
    "\n",
    "    # Step 3: print class counts\n",
    "    for cls in sorted(class_map.keys()):\n",
    "        print(f\" Class {cls}: selected {len(class_map[cls])} patches\")\n",
    "\n",
    "    # Step 4‚Äì6: shuffle, split, build datasets\n",
    "    indices = [item for lst in class_map.values() for item in lst]\n",
    "    random.seed(seed)\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    n_total = len(indices)\n",
    "    n_train = int(train_ratio * n_total)\n",
    "    n_val = int(val_ratio * n_total)\n",
    "\n",
    "    train_idx = indices[:n_train]\n",
    "    val_idx = indices[n_train:n_train + n_val]\n",
    "    test_idx = indices[n_train + n_val:]\n",
    "\n",
    "    train_ds = HYPSOPatchDataset(cube_data, train_idx, band_means, band_stds, patch_size, augment=True, majority_label=majority_label)\n",
    "    val_ds = HYPSOPatchDataset(cube_data, val_idx, band_means, band_stds, patch_size, augment=False, majority_label=majority_label)\n",
    "    test_ds = HYPSOPatchDataset(cube_data, test_idx, band_means, band_stds, patch_size, augment=False, majority_label=majority_label)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375da6f6",
   "metadata": {},
   "source": [
    "### With Mapping Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a57bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = torch.load('/home/salyken/PRISMA/HYPSO_data/HYPSO_dataset_processed/mean_std/mean_std.pt', weights_only=False)\n",
    "\n",
    "# Access tensors\n",
    "band_means = stats['band_means']\n",
    "band_stds = stats['band_stds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b700d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Shape\n",
    "print(band_means.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f352a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = create_patch_datasets(\n",
    "    cube_dir = cube_path, label_dir=labels_path, cube_list=cube_files,\n",
    "    band_means=band_means, band_stds=band_stds,\n",
    "    patch_size=71, stride=15,\n",
    "    train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,\n",
    "    seed=42,\n",
    "    majority_label=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d145a6a",
   "metadata": {},
   "source": [
    "### With Projection Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecd1ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_stats = torch.load('/home/salyken/PRISMA/HYPSO_data/HYPSO_dataset_processed/mean_std/projection_mean_std_47.pt', weights_only=False)\n",
    "\n",
    "# Access tensors\n",
    "projection_band_means = projection_stats['band_means']\n",
    "projection_band_stds = projection_stats['band_stds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape\n",
    "print(projection_band_means.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.load(\"/home/salyken/PRISMA/hypso_to_prisma_projection/hypso_to_prisma_projection.npy\") # shape: (47,117)\n",
    "print(W.shape)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = create_patch_datasets(\n",
    "    cube_dir = cube_path, label_dir=labels_path, cube_list=cube_files,\n",
    "    band_means=projection_band_means, band_stds=projection_band_stds,\n",
    "    patch_size=71, stride=15,\n",
    "    train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,\n",
    "    seed=42,\n",
    "    projection_matrix = W,\n",
    "    majority_label = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383155f",
   "metadata": {},
   "source": [
    "### Training Loop for Pretrained PRISMA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2091fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to change backbone freezing for each model\n",
    "\n",
    "# EarlyStopping Helper\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=70, min_delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Unified transfer learning trainer\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    train_dataset,\n",
    "    device,\n",
    "    checkpoint_path,\n",
    "    save_dir,\n",
    "    resume=True,\n",
    "    start_epoch=0,\n",
    "    num_epochs=100,\n",
    "    head_lr=1e-3,\n",
    "    backbone_lr=2e-4,\n",
    "    unfreeze_epoch=8,\n",
    "    scheduler_patience=3,\n",
    "    scheduler_factor=0.5\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    chkpt_file = os.path.join(save_dir, \"checkpoint.pth\")\n",
    "    print('Hypso save directory:', chkpt_file)\n",
    "    print(\" File already exists:\", os.path.exists(chkpt_file))\n",
    "\n",
    "    if resume and os.path.exists(chkpt_file):\n",
    "        print(f\" Resuming training from: {chkpt_file}\")\n",
    "        resume_ckpt = torch.load(chkpt_file, map_location=device)\n",
    "        model.load_state_dict(resume_ckpt['model_state_dict'])\n",
    "        optimizer_state = resume_ckpt.get('optimizer_state_dict')\n",
    "\n",
    "        start_epoch = resume_ckpt.get('epoch', start_epoch) + 1\n",
    "    else:\n",
    "        print(f\" Loading pretrained checkpoint from: {checkpoint_path}\")\n",
    "        ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "        state_dict = ckpt.get('model_state_dict', ckpt)\n",
    "\n",
    "        new_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            if k.startswith(\"module.\"):\n",
    "                k = k[len(\"module.\"):]\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "        excluded_prefixes = ['mapping.', 'classifier.']\n",
    "        filtered = {\n",
    "            k: v for k, v in new_state_dict.items()\n",
    "            if not any(k.startswith(prefix) for prefix in excluded_prefixes)\n",
    "        }\n",
    "\n",
    "        model_dict = model.state_dict()\n",
    "        compatible_weights = {\n",
    "            k: v for k, v in filtered.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "\n",
    "        model_dict.update(compatible_weights)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        print(f\" Loaded {len(compatible_weights)} compatible pretrained layers.\")\n",
    "        skipped = [k for k in filtered if k not in compatible_weights]\n",
    "        if skipped:\n",
    "            print(f\" Skipped {len(skipped)} incompatible layers (shape mismatch):\")\n",
    "            for k in skipped:\n",
    "                print(f\" - {k} (saved shape: {filtered[k].shape}, model shape: {model_dict.get(k, 'N/A')})\")\n",
    "\n",
    "        optimizer_state = None\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # VGG-16 CBAM:\n",
    "    backbone_prefixes = ['block1', 'block2', 'block3', 'block4', 'block5']\n",
    "    # VGG-16 sSE:\n",
    "    # backbone_prefixes = [f'blocks.{i}' for i in range(5)]\n",
    "    # VGG-16:\n",
    "    # backbone_prefixes = [f\"features.{i}\" for i in range(0, 43)]\n",
    "    # Hybrid: \n",
    "    # backbone_prefixes = [\n",
    "    #     *[f\"encoder3d.{i}\" for i in range(5)],  # freeze early 3D conv layers\n",
    "    #     *[f\"features2d.{i}\" for i in range(11)]  # 2D CNN head\n",
    "    # ]\n",
    "    # 2D: \n",
    "    # backbone_prefixes = [f\"features.{i}\" for i in range(0, 3)]\n",
    "\n",
    "\n",
    "\n",
    "    if resume and start_epoch > unfreeze_epoch:\n",
    "        print(f\"‚Ü™ Resuming after unfreeze epoch; keeping backbone unfrozen\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(name.startswith(p) for p in backbone_prefixes):\n",
    "                param.requires_grad = True\n",
    "        print(\"\\nüîì Re-checking trainable parameters after unfreeze:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            status = \"‚úÖ trainable\" if param.requires_grad else \"‚ùå frozen\"\n",
    "            print(f\"{name:50s} | {status}\")\n",
    "    else:\n",
    "        print(f\"‚Ü™ Freezing backbone until epoch {unfreeze_epoch}\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(name.startswith(p) for p in backbone_prefixes):\n",
    "                param.requires_grad = False\n",
    "\n",
    "    print(\"\\nüîç Trainable Parameters Summary:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        status = \"‚úÖ trainable\" if param.requires_grad else \"‚ùå frozen\"\n",
    "        print(f\"{name:50s} | {status}\")\n",
    "\n",
    "    if hasattr(train_dataset, 'class_counts'):\n",
    "        counts = np.array([train_dataset.class_counts[c] for c in sorted(train_dataset.class_counts)])\n",
    "    else:\n",
    "        print(\"‚öñÔ∏è Estimating class weights from the full training dataset...\")\n",
    "        all_labels = []\n",
    "        for idx in tqdm(range(len(train_dataset)), desc=\"Scanning dataset\"):\n",
    "            try:\n",
    "                label = int(train_dataset[idx][1])\n",
    "                all_labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\" Error on sample {idx}: {e}\")\n",
    "\n",
    "        if len(all_labels) == 0:\n",
    "            raise RuntimeError(\"No valid labels found ‚Äî check your dataset logic!\")\n",
    "\n",
    "        num_classes = model.num_classes if hasattr(model, 'num_classes') else len(set(all_labels))\n",
    "        counts = np.bincount(all_labels, minlength=num_classes)\n",
    "\n",
    "    weights = counts.max() / counts\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    head_params = [p for n, p in model.named_parameters() if not any(n.startswith(b) for b in backbone_prefixes)]\n",
    "    backbone_params = [p for n, p in model.named_parameters() if any(n.startswith(b) for b in backbone_prefixes)]\n",
    "\n",
    "    if optimizer_state:\n",
    "        saved_param_groups = optimizer_state.get(\"param_groups\", [])\n",
    "        if len(saved_param_groups) == 1:\n",
    "            print(\"üîÅ Restoring optimizer with 1 param group (head only)\")\n",
    "            optimizer = optim.Adam(\n",
    "                [{'params': head_params, 'lr': head_lr}],\n",
    "                weight_decay=1e-5\n",
    "            )\n",
    "        else:\n",
    "            print(\"üîÅ Restoring optimizer with 2 param groups (head + backbone)\")\n",
    "            optimizer = optim.Adam(\n",
    "                [\n",
    "                    {'params': head_params, 'lr': head_lr},\n",
    "                    {'params': backbone_params, 'lr': backbone_lr}\n",
    "                ],\n",
    "                weight_decay=1e-5\n",
    "            )\n",
    "    else:\n",
    "        print(\"‚öôÔ∏è Initializing fresh optimizer with 2 param groups\")\n",
    "        optimizer = optim.Adam(\n",
    "            [\n",
    "                {'params': head_params, 'lr': head_lr},\n",
    "                {'params': backbone_params, 'lr': backbone_lr}\n",
    "            ],\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "\n",
    "    if optimizer_state:\n",
    "        try:\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "            print(\" Optimizer state successfully restored.\")\n",
    "        except ValueError as e:\n",
    "            print(f\" Optimizer state mismatch: {e}\")\n",
    "            print(\" Proceeding with freshly initialized optimizer.\")\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience)\n",
    "    best_val_acc = float('-inf')\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        if epoch == unfreeze_epoch:\n",
    "            print(f\" Unfreezing backbone at epoch {epoch}\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(name.startswith(p) for p in backbone_prefixes):\n",
    "                    param.requires_grad = True\n",
    "            backbone_params = [p for n, p in model.named_parameters() if any(n.startswith(b) for b in backbone_prefixes)]\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=True):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * targets.size(0)\n",
    "            train_correct += (outputs.argmax(1) == targets).sum().item()\n",
    "            train_total += targets.size(0)\n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validating\", leave=True):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * targets.size(0)\n",
    "                val_correct += (outputs.argmax(1) == targets).sum().item()\n",
    "                val_total += targets.size(0)\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train: {train_loss:.4f}, {train_acc:.4f} | Val: {val_loss:.4f}, {val_acc:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_path = os.path.join(save_dir, \"best_val_acc.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, best_path)\n",
    "            print(f\" Saved best model at epoch {epoch+1} with val acc {val_acc:.4f} ‚Üí {best_path}\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "        }, chkpt_file)\n",
    "\n",
    "    print(\"üèÅ Training complete; best val acc:\", best_val_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9ed48",
   "metadata": {},
   "source": [
    "### Training Loop From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84364725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_without_pretrained_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    train_dataset,\n",
    "    device,\n",
    "    checkpoint_path,\n",
    "    save_dir,\n",
    "    resume=False,  # default to False for scratch training\n",
    "    start_epoch=0,\n",
    "    num_epochs=100,\n",
    "    head_lr=1e-3,\n",
    "    backbone_lr=2e-4,  # kept in case you later want to use two groups again\n",
    "    unfreeze_epoch=8,\n",
    "    scheduler_patience=3,\n",
    "    scheduler_factor=0.5\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    chkpt_file = os.path.join(save_dir, \"checkpoint.pth\")\n",
    "    print('Hypso save directory:', chkpt_file)\n",
    "    print(\" File already exists:\", os.path.exists(chkpt_file))\n",
    "\n",
    "    if resume and os.path.exists(chkpt_file):\n",
    "        print(f\"üîÑ Resuming training from: {chkpt_file}\")\n",
    "        resume_ckpt = torch.load(chkpt_file, map_location=device)\n",
    "        model.load_state_dict(resume_ckpt['model_state_dict'])\n",
    "        optimizer_state = resume_ckpt.get('optimizer_state_dict')\n",
    "        start_epoch = resume_ckpt.get('epoch', start_epoch) + 1\n",
    "    else:\n",
    "        print(\" Training from scratch ‚Äî no pretrained weights loaded.\")\n",
    "        optimizer_state = None\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    print(\" Training all parameters (no freezing needed for scratch training)\")\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    print(\"\\nüîç Trainable Parameters Summary:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        status = \"‚úÖ trainable\" if param.requires_grad else \"‚ùå frozen\"\n",
    "        print(f\"{name:50s} | {status}\")\n",
    "\n",
    "    if hasattr(train_dataset, 'class_counts'):\n",
    "        counts = np.array([train_dataset.class_counts[c] for c in sorted(train_dataset.class_counts)])\n",
    "    else:\n",
    "        print(\"‚öñÔ∏è Estimating class weights from the full training dataset...\")\n",
    "        all_labels = []\n",
    "        for idx in tqdm(range(len(train_dataset)), desc=\"Scanning dataset\"):\n",
    "            try:\n",
    "                label = int(train_dataset[idx][1])\n",
    "                all_labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\" Error on sample {idx}: {e}\")\n",
    "\n",
    "        if len(all_labels) == 0:\n",
    "            raise RuntimeError(\"No valid labels found ‚Äî check your dataset logic!\")\n",
    "\n",
    "        num_classes = model.num_classes if hasattr(model, 'num_classes') else len(set(all_labels))\n",
    "        counts = np.bincount(all_labels, minlength=num_classes)\n",
    "\n",
    "    weights = counts.max() / counts\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    if optimizer_state:\n",
    "        print(\" Restoring optimizer\")\n",
    "        optimizer = optim.Adam(model.parameters(), lr=head_lr, weight_decay=1e-5)\n",
    "    else:\n",
    "        print(\" Initializing fresh optimizer (all parameters)\")\n",
    "        optimizer = optim.Adam(model.parameters(), lr=head_lr, weight_decay=1e-5)\n",
    "\n",
    "    if optimizer_state:\n",
    "        try:\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "            print(\" Optimizer state successfully restored.\")\n",
    "        except ValueError as e:\n",
    "            print(f\" Optimizer state mismatch: {e}\")\n",
    "            print(\" Proceeding with freshly initialized optimizer.\")\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience)\n",
    "    best_val_acc = float('-inf')\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=True):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * targets.size(0)\n",
    "            train_correct += (outputs.argmax(1) == targets).sum().item()\n",
    "            train_total += targets.size(0)\n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validating\", leave=True):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * targets.size(0)\n",
    "                val_correct += (outputs.argmax(1) == targets).sum().item()\n",
    "                val_total += targets.size(0)\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train: {train_loss:.4f}, {train_acc:.4f} | Val: {val_loss:.4f}, {val_acc:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_path = os.path.join(save_dir, \"best_val_acc.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, best_path)\n",
    "            print(f\" Saved best model at epoch {epoch+1} with val acc {val_acc:.4f} ‚Üí {best_path}\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "        }, chkpt_file)\n",
    "\n",
    "    print(\"üèÅ Training complete; best val acc:\", best_val_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5f4aa",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b59fefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16WithCBAM(in_channels=117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d1de200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([117, 71, 71])\n"
     ]
    }
   ],
   "source": [
    "sample, label = train_dataset[0]\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "prisma_ckpt = \"/home/salyken/PRISMA/PRISMA_data/PRISMA_dataset_processed/model/VGG16_w_cbam_71_patch_clipped/best_val_acc.pth\"\n",
    "save_dir = \"/home/salyken/PRISMA/HYPSO_data/HYPSO_dataset_processed/models/VGG16_w_cbam_71_patch_clipped\"\n",
    "\n",
    "train_model(model, train_loader, val_loader, train_dataset, device, prisma_ckpt, save_dir, resume=False, unfreeze_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d9755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "prisma_ckpt = \"/home/salyken/PRISMA/PRISMA_data/PRISMA_dataset_processed/model/VGG16_w_cbam_71_patch_clipped/best_val_acc.pth\"\n",
    "save_dir = \"/home/salyken/PRISMA/HYPSO_data/HYPSO_dataset_processed/models/VGG16_w_cbam_71_patch_clipped\"\n",
    "\n",
    "train_without_pretrained_model(\n",
    "    model=model,  \n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    train_dataset=train_dataset,\n",
    "    device=device,\n",
    "    checkpoint_path=None,  # not used for scratch training\n",
    "    save_dir=save_dir,  \n",
    "    resume=False,  \n",
    "    num_epochs=100,  \n",
    "    head_lr=1e-3  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
